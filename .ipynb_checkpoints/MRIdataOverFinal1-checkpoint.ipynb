{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb19c578-1075-45d9-a8ad-c02539a2b88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found: TRAIN\n",
      "✅ Found: VAL\n",
      "✅ Found: TEST\n",
      "✅ Dataset structure verified. Now ready for image processing!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define dataset root path\n",
    "dataset_path = r\"C:\\Users\\Suraj Yadav\\FinalProjectTest\\datasetMRI\\Br35H-Mask-RCNN\"\n",
    "\n",
    "# Define processed image paths\n",
    "processed_folder = os.path.join(dataset_path, \"Processed_Images\")\n",
    "image_folder = os.path.join(processed_folder, \"images\")\n",
    "mask_folder = os.path.join(processed_folder, \"masks\")\n",
    "\n",
    "# ✅ Create necessary directories if they don’t exist\n",
    "os.makedirs(processed_folder, exist_ok=True)\n",
    "os.makedirs(image_folder, exist_ok=True)\n",
    "os.makedirs(mask_folder, exist_ok=True)\n",
    "\n",
    "# ✅ Verify dataset structure\n",
    "folders = [\"TRAIN\", \"VAL\", \"TEST\"]\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "    if os.path.exists(folder_path):\n",
    "        print(f\"✅ Found: {folder}\")\n",
    "    else:\n",
    "        print(f\"❌ Missing: {folder}\")\n",
    "\n",
    "print(\"✅ Dataset structure verified. Now ready for image processing!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bf53a1f-e570-4e76-b79e-497023a7ae07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All original images saved in: C:\\Users\\Suraj Yadav\\FinalProjectTest\\datasetMRI\\Br35H-Mask-RCNN\\Processed_Images\\original\n",
      "✅ All overlay images saved in: C:\\Users\\Suraj Yadav\\FinalProjectTest\\datasetMRI\\Br35H-Mask-RCNN\\Processed_Images\\overlay\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.image import save_img\n",
    "\n",
    "# ✅ Define dataset directories\n",
    "dataset_path = r\"C:\\Users\\Suraj Yadav\\FinalProjectTest\\datasetMRI\\Br35H-Mask-RCNN\"\n",
    "processed_folder = os.path.join(dataset_path, \"Processed_Images\")\n",
    "original_folder = os.path.join(processed_folder, \"original\")\n",
    "overlay_folder = os.path.join(processed_folder, \"overlay\")\n",
    "\n",
    "# ✅ Create folders if they don’t exist\n",
    "os.makedirs(original_folder, exist_ok=True)\n",
    "os.makedirs(overlay_folder, exist_ok=True)\n",
    "\n",
    "# ✅ Load annotation JSON\n",
    "annotation_file = os.path.join(dataset_path, \"annotations_all.json\")\n",
    "\n",
    "with open(annotation_file, \"r\") as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# ✅ Function to create mask from annotation\n",
    "def create_mask_from_annotation(annotation, original_size):\n",
    "    h_orig, w_orig = original_size  \n",
    "    mask = np.zeros((h_orig, w_orig), dtype=np.uint8)\n",
    "    contour_list = []\n",
    "\n",
    "    for region in annotation[\"regions\"]:\n",
    "        shape_attr = region[\"shape_attributes\"]\n",
    "        \n",
    "        if shape_attr[\"name\"] == \"polygon\":\n",
    "            all_x = np.array(shape_attr[\"all_points_x\"])\n",
    "            all_y = np.array(shape_attr[\"all_points_y\"])\n",
    "\n",
    "            all_x = np.clip(all_x, 0, w_orig - 1)\n",
    "            all_y = np.clip(all_y, 0, h_orig - 1)\n",
    "\n",
    "            contour = np.array(list(zip(all_x, all_y)), dtype=np.int32)\n",
    "            contour_list.append(contour)\n",
    "\n",
    "            # ✅ Draw the tumor mask properly\n",
    "            cv2.fillPoly(mask, [contour], 255)\n",
    "\n",
    "    return mask, contour_list\n",
    "\n",
    "# ✅ Process all images from TRAIN, TEST, and VAL folders\n",
    "for folder in [\"TRAIN\", \"TEST\", \"VAL\"]:\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "\n",
    "    for img_name in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, img_name)\n",
    "\n",
    "        # ✅ Check if it's a valid image\n",
    "        if not img_name.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "            continue\n",
    "\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            print(f\"⚠️ Skipping {img_name}, unable to read.\")\n",
    "            continue\n",
    "\n",
    "        original_size = img.shape[:2]  \n",
    "\n",
    "        # ✅ Save original image\n",
    "        save_img(os.path.join(original_folder, img_name), np.expand_dims(img, axis=-1))\n",
    "\n",
    "        base_filename = os.path.splitext(img_name)[0]  \n",
    "        matching_key = next((key for key in annotations.keys() if base_filename in key), None)\n",
    "\n",
    "        if matching_key:\n",
    "            mask, contours = create_mask_from_annotation(annotations[matching_key], original_size)\n",
    "\n",
    "            # ✅ Fix color formatting\n",
    "            img_color = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "            # ✅ Create mask overlay\n",
    "            colored_mask = np.zeros_like(img_color)\n",
    "            colored_mask[:, :, 2] = mask  \n",
    "\n",
    "            # ✅ Blend overlay\n",
    "            overlayed_image = cv2.addWeighted(img_color, 1, colored_mask, 0.6, 0)\n",
    "\n",
    "            # ✅ Draw tumor boundary in **Blue**\n",
    "            cv2.polylines(overlayed_image, contours, isClosed=True, color=(0, 0, 255), thickness=2)\n",
    "\n",
    "            # ✅ Add tumor label at correct position\n",
    "            if len(contours) > 0:\n",
    "                M = cv2.moments(contours[0])\n",
    "                if M[\"m00\"] != 0:\n",
    "                    cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "                    cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "                    cv2.putText(overlayed_image, \"Tumor Detected\", (cX - 10, cY - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 100, 10), 2, cv2.LINE_AA)\n",
    "\n",
    "            # ✅ Save overlay image\n",
    "            save_img(os.path.join(overlay_folder, img_name), overlayed_image)\n",
    "\n",
    "print(f\"✅ All original images saved in: {original_folder}\")\n",
    "print(f\"✅ All overlay images saved in: {overlay_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649c97d9-58e8-4b75-98de-5816e6146b51",
   "metadata": {},
   "source": [
    "Processing All Images (Original + Overlayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99c1ec3d-215a-4a82-8cc1-dd224493c781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All resized original images saved in: C:\\Users\\Suraj Yadav\\FinalProjectTest\\datasetMRI\\Br35H-Mask-RCNN\\Processed_Images\\final_original\n",
      "✅ All resized overlay images saved in: C:\\Users\\Suraj Yadav\\FinalProjectTest\\datasetMRI\\Br35H-Mask-RCNN\\Processed_Images\\final_overlay\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import save_img\n",
    "\n",
    "# ✅ Define dataset directories\n",
    "processed_folder = os.path.join(dataset_path, \"Processed_Images\")\n",
    "original_folder = os.path.join(processed_folder, \"original\")\n",
    "overlay_folder = os.path.join(processed_folder, \"overlay\")\n",
    "final_original_folder = os.path.join(processed_folder, \"final_original\")\n",
    "final_overlay_folder = os.path.join(processed_folder, \"final_overlay\")\n",
    "\n",
    "# ✅ Create necessary directories\n",
    "os.makedirs(final_original_folder, exist_ok=True)\n",
    "os.makedirs(final_overlay_folder, exist_ok=True)\n",
    "\n",
    "# ✅ Target size (256x256) - Ensuring consistency\n",
    "target_size = (256, 256)\n",
    "\n",
    "# ✅ Function to resize both images while preserving alignment\n",
    "def resize_pair(image, overlay, target_size):\n",
    "    image_resized = cv2.resize(image, target_size, interpolation=cv2.INTER_NEAREST)\n",
    "    overlay_resized = cv2.resize(overlay, target_size, interpolation=cv2.INTER_NEAREST)\n",
    "    return image_resized, overlay_resized\n",
    "\n",
    "# ✅ Process all images in the original and overlay folders\n",
    "for img_name in os.listdir(original_folder):\n",
    "    img_path = os.path.join(original_folder, img_name)\n",
    "    overlay_path = os.path.join(overlay_folder, img_name)\n",
    "\n",
    "    # ✅ Load images\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    overlay = cv2.imread(overlay_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    if img is None or overlay is None:\n",
    "        print(f\"⚠️ Skipping {img_name}, unable to read.\")\n",
    "        continue\n",
    "\n",
    "    # ✅ Resize both images together\n",
    "    img_resized, overlay_resized = resize_pair(img, overlay, target_size)\n",
    "\n",
    "    # ✅ Save resized images\n",
    "    save_img(os.path.join(final_original_folder, img_name), np.expand_dims(img_resized, axis=-1))\n",
    "    save_img(os.path.join(final_overlay_folder, img_name), np.expand_dims(overlay_resized, axis=-1))\n",
    "\n",
    "print(f\"✅ All resized original images saved in: {final_original_folder}\")\n",
    "print(f\"✅ All resized overlay images saved in: {final_overlay_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86c3bd22-50bd-47a7-8df1-16ffff2c869a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All images resized and stored in:\n",
      "   📂 C:\\Users\\Suraj Yadav\\FinalProjectTest\\datasetMRI\\Br35H-Mask-RCNN\\Processed_Images\\resized_original\n",
      "   📂 C:\\Users\\Suraj Yadav\\FinalProjectTest\\datasetMRI\\Br35H-Mask-RCNN\\Processed_Images\\resized_overlays\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import save_img\n",
    "\n",
    "# ✅ Define dataset directories\n",
    "base_folder = r\"C:\\Users\\Suraj Yadav\\FinalProjectTest\\datasetMRI\\Br35H-Mask-RCNN\"\n",
    "processed_folder = os.path.join(base_folder, \"Processed_Images\")\n",
    "\n",
    "# ✅ Define paths for resized images\n",
    "resized_original_folder = os.path.join(processed_folder, \"resized_original\")\n",
    "resized_overlays_folder = os.path.join(processed_folder, \"resized_overlays\")\n",
    "\n",
    "# ✅ Define paths for final processed images\n",
    "final_images_folder = os.path.join(processed_folder, \"final_images\")\n",
    "final_original_folder = os.path.join(final_images_folder, \"images_resized_original\")\n",
    "final_masked_folder = os.path.join(final_images_folder, \"images_masked_overlays\")\n",
    "\n",
    "# ✅ Create necessary directories\n",
    "os.makedirs(resized_original_folder, exist_ok=True)\n",
    "os.makedirs(resized_overlays_folder, exist_ok=True)\n",
    "os.makedirs(final_original_folder, exist_ok=True)\n",
    "os.makedirs(final_masked_folder, exist_ok=True)\n",
    "\n",
    "# ✅ Target size for all images\n",
    "img_size = (256, 256)\n",
    "\n",
    "# ✅ Function to resize images\n",
    "def resize_image(image_path, output_path, img_size, is_gray=True):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE if is_gray else cv2.IMREAD_COLOR)\n",
    "    if img is not None:\n",
    "        resized_img = cv2.resize(img, img_size)\n",
    "        save_img(output_path, np.expand_dims(resized_img, axis=-1) if is_gray else resized_img)\n",
    "    else:\n",
    "        print(f\"⚠️ Warning: Unable to load {image_path}\")\n",
    "\n",
    "# ✅ Resize original images\n",
    "original_folder = os.path.join(processed_folder, \"original\")\n",
    "for img_name in os.listdir(original_folder):\n",
    "    input_path = os.path.join(original_folder, img_name)\n",
    "    output_path = os.path.join(resized_original_folder, img_name)\n",
    "    resize_image(input_path, output_path, img_size, is_gray=True)\n",
    "\n",
    "# ✅ Resize overlay images\n",
    "overlay_folder = os.path.join(processed_folder, \"overlay\")\n",
    "for img_name in os.listdir(overlay_folder):\n",
    "    input_path = os.path.join(overlay_folder, img_name)\n",
    "    output_path = os.path.join(resized_overlays_folder, img_name)\n",
    "    resize_image(input_path, output_path, img_size, is_gray=False)\n",
    "\n",
    "print(f\"✅ All images resized and stored in:\")\n",
    "print(f\"   📂 {resized_original_folder}\")\n",
    "print(f\"   📂 {resized_overlays_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a90f0ddf-cb22-4aa8-aad3-62da82c02ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All binary masks generated and saved in: C:\\Users\\Suraj Yadav\\FinalProjectTest\\datasetMRI\\Br35H-Mask-RCNN\\Processed_Images\\masks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.image import save_img\n",
    "\n",
    "# ✅ Define dataset directories\n",
    "base_folder = r\"C:\\Users\\Suraj Yadav\\FinalProjectTest\\datasetMRI\\Br35H-Mask-RCNN\"\n",
    "processed_folder = os.path.join(base_folder, \"Processed_Images\")\n",
    "\n",
    "# ✅ Paths for images and masks\n",
    "original_folder = os.path.join(processed_folder, \"original\")  # Use ORIGINAL images\n",
    "masks_folder = os.path.join(processed_folder, \"masks\")  # ✅ Binary masks will be stored here\n",
    "\n",
    "# ✅ Create necessary directories\n",
    "os.makedirs(masks_folder, exist_ok=True)\n",
    "\n",
    "# ✅ Load annotation JSON\n",
    "annotation_file = os.path.join(base_folder, \"annotations_all.json\")\n",
    "with open(annotation_file, \"r\") as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# ✅ Function to create mask from annotation\n",
    "def create_mask_from_annotation(annotation, img_shape):\n",
    "    h_orig, w_orig = img_shape  # Get original image dimensions\n",
    "    mask = np.zeros((h_orig, w_orig), dtype=np.uint8)  # Black background\n",
    "\n",
    "    for region in annotation[\"regions\"]:\n",
    "        shape_attr = region[\"shape_attributes\"]\n",
    "        \n",
    "        if shape_attr[\"name\"] == \"polygon\":\n",
    "            all_x = np.array(shape_attr[\"all_points_x\"])\n",
    "            all_y = np.array(shape_attr[\"all_points_y\"])\n",
    "\n",
    "            # Ensure coordinates stay within the image bounds\n",
    "            all_x = np.clip(all_x, 0, w_orig - 1)\n",
    "            all_y = np.clip(all_y, 0, h_orig - 1)\n",
    "\n",
    "            # Convert points to contour format\n",
    "            contour = np.array(list(zip(all_x, all_y)), dtype=np.int32)\n",
    "\n",
    "            # ✅ Fill the tumor region with white (255)\n",
    "            cv2.fillPoly(mask, [contour], 255)\n",
    "\n",
    "    return mask\n",
    "\n",
    "# ✅ Process all images from the original folder\n",
    "for img_name in os.listdir(original_folder):\n",
    "    img_path = os.path.join(original_folder, img_name)\n",
    "    mask_path = os.path.join(masks_folder, img_name)\n",
    "\n",
    "    # ✅ Load original image to get dimensions\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    if img is None:\n",
    "        print(f\"⚠️ Skipping {img_name}, unable to read.\")\n",
    "        continue\n",
    "\n",
    "    # ✅ Extract corresponding annotation key\n",
    "    base_filename = os.path.splitext(img_name)[0]  \n",
    "    matching_key = next((key for key in annotations.keys() if base_filename in key), None)\n",
    "\n",
    "    if matching_key:\n",
    "        # ✅ Generate tumor mask\n",
    "        mask = create_mask_from_annotation(annotations[matching_key], img.shape)\n",
    "\n",
    "        # ✅ Save the generated binary mask\n",
    "        save_img(mask_path, np.expand_dims(mask, axis=-1))\n",
    "\n",
    "print(f\"✅ All binary masks generated and saved in: {masks_folder}\")\n",
    "# isse masking perfect ho raha hai with original size images ab bus isko resize krna hai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0e5face-d928-4930-9494-ffaab4697fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All resized masks stored in: C:\\Users\\Suraj Yadav\\FinalProjectTest\\datasetMRI\\Br35H-Mask-RCNN\\Processed_Images\\resized_masks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import save_img\n",
    "\n",
    "# ✅ Paths for resized masks\n",
    "resized_masks_folder = os.path.join(processed_folder, \"resized_masks\")  # ✅ Final aligned masks\n",
    "os.makedirs(resized_masks_folder, exist_ok=True)\n",
    "\n",
    "# ✅ Target size\n",
    "target_size = (256, 256)\n",
    "\n",
    "# ✅ Function to resize while preserving aspect ratio\n",
    "def resize_with_aspect_ratio(image, target_size, interpolation=cv2.INTER_NEAREST):\n",
    "    h, w = image.shape[:2]\n",
    "    target_w, target_h = target_size\n",
    "\n",
    "    # ✅ Compute scale to maintain aspect ratio\n",
    "    scale = min(target_w / w, target_h / h)\n",
    "    new_w = int(w * scale)\n",
    "    new_h = int(h * scale)\n",
    "\n",
    "    # ✅ Resize image\n",
    "    resized = cv2.resize(image, (new_w, new_h), interpolation=interpolation)\n",
    "\n",
    "    # ✅ Create a black canvas\n",
    "    final_image = np.zeros((target_h, target_w), dtype=np.uint8)\n",
    "\n",
    "    # ✅ Center the resized image on the black canvas\n",
    "    x_offset = (target_w - new_w) // 2\n",
    "    y_offset = (target_h - new_h) // 2\n",
    "    final_image[y_offset:y_offset + new_h, x_offset:x_offset + new_w] = resized\n",
    "\n",
    "    return final_image\n",
    "\n",
    "# ✅ Resize masks while preserving aspect ratio\n",
    "masks_folder = os.path.join(processed_folder, \"masks\")  # Previously generated masks\n",
    "\n",
    "for mask_name in os.listdir(masks_folder):\n",
    "    mask_path = os.path.join(masks_folder, mask_name)\n",
    "    resized_mask_path = os.path.join(resized_masks_folder, mask_name)\n",
    "\n",
    "    # ✅ Load mask\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    if mask is not None:\n",
    "        # ✅ Resize mask while maintaining aspect ratio\n",
    "        resized_mask = resize_with_aspect_ratio(mask, target_size)\n",
    "\n",
    "        # ✅ Save resized mask\n",
    "        save_img(resized_mask_path, np.expand_dims(resized_mask, axis=-1))\n",
    "    else:\n",
    "        print(f\"⚠️ Warning: Unable to read {mask_name}\")\n",
    "\n",
    "print(f\"✅ All resized masks stored in: {resized_masks_folder}\")\n",
    "# 256 by 256 pixels me properly resize ho chuka hai wo bhi perfectly aligned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9d746a-1fff-4cbc-8117-a133ced98108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a5072e-f456-43cf-b749-181369613966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e0eb49-7493-4531-a0d5-d3367c8ffe23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f884f7e2-861a-4a13-b12e-e1eec1801945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8057e961-d1e6-4c7a-b0b8-02065c1e0eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522c6902-bd80-4034-8d75-6fe9295e5e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872f563d-887c-4e15-9731-efa7aa1d9ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32e456e5-1ce6-4036-a652-b313cb88af56",
   "metadata": {},
   "source": [
    "Splitting Dataset into Train/Val/Test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
